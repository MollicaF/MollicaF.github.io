[
  {
    "objectID": "DataAnalysisModel.html",
    "href": "DataAnalysisModel.html",
    "title": "Data Analysis Model",
    "section": "",
    "text": "So rather than grid-sweeping the temperatures, let’s infer them from the data.\nSo let’s translate our model function into stan code. The \\(\\tau\\) parameters range from 0 to \\(\\infty\\) but the relevant part of the parameter space is really between 0 and 1. So let’s transform the random variable \\(x\\) following \\(\\tau=2^x\\) and put a prior over \\(x\\) as: \\[ x \\sim N(0, 0.1)\\].\n\nfunctions {\n\n  vector center(vector x) {\n    return x - mean(x);\n  }\n\n  vector eye_cost(vector p) {\n    int leng = num_elements(p);\n    matrix[leng, leng] look;\n    look = rep_matrix(0, leng, leng);\n\n    for (i in 1:4) {\n      look[i, 1] = 1 * p[i];\n      real pj = 1 - p[i];\n      for (j in 1:4) {\n        if (j != i) {\n          real pk = pj - p[j];\n          look[j, 2] += 2 * p[i] * p[j] / pj;\n          for (k in 1:4) {\n            if (k != i && k != j) {\n              real pl = pk - p[k];\n              look[k, 3] += 3 * p[i] * p[j] / pj * p[k] / pk;\n              for (l in 1:4) {\n                if (l != i && l != j && l != k) {\n                  look[l, 4] += 4 * p[i] * p[j] / pj * p[k] / pk * p[l] / pl;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    vector[leng] cost;\n    for (i in 1:leng) {\n      cost[i] = 3 - sum(look[i, ]); // row sum of the look matrix\n    }\n    return cost;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; W; // words\n  int&lt;lower=0&gt; R; // referents\n  int&lt;lower=0&gt; K; // Items\n  int&lt;lower=0&gt; N_resp; // number of trials\n  matrix[R, W] distance[K];\n  int Ki[N_resp]; // item index\n  int ref[N_resp]; // referent index\n  int y[N_resp, W]; // w responses\n}\n\nparameters {\n  real stau_lap;\n  real ltau_lap;\n}\n\ntransformed parameters {\n  real&lt;lower=0&gt; stau;\n  real&lt;lower=0&gt; ltau;\n  matrix[R, W] L0[K];\n  matrix[R, W] cost_l0[K];\n  matrix[W, R] S0[K];\n  matrix[R, W] L1[K];\n  matrix[R, W] cost_l1[K];\n  matrix[W, R] S1[K];\n\n  stau = pow(2, stau_lap);\n  ltau = pow(2, ltau_lap);\n  for(k in 1:K){\n    for(w in 1:W){\n      L0[k][,w] = softmax(center(to_vector(distance[k][,w])/ltau));\n    }\n    for(w in 1:W){\n      cost_l0[k][,w] = eye_cost(to_vector(L0[k][,w]));\n    }\n    for(r in 1:R){\n      S0[k][,r] = softmax(to_vector(cost_l0[k][r,])/stau);\n    }\n    for(w in 1:W){\n      L1[k][,w] = softmax(log(to_vector(S0[k][w,]))/ltau);\n    }\n    for(w in 1:W){\n      cost_l1[k][,w] = eye_cost(to_vector(L1[k][,w]));\n    }\n    for(r in 1:R){\n      S1[k][,r] = softmax(to_vector(cost_l1[k][r,])/stau);\n    }\n  }\n}\n\nmodel {\n  // priors\n  target += normal_lpdf(stau_lap | 0, 0.1);\n  target += normal_lpdf(ltau_lap | 0, 0.1);\n\n  // likelihood\n  for(n in 1:N_resp){\n      target +=  multinomial_lpmf(y[n,] | S1[Ki[n]][, ref[n]]);\n  }\n}\n\ngenerated quantities {\n  int y_rep[N_resp, W]; // w responses\n  for(n in 1:N_resp){\n    y_rep[n,] = multinomial_rng(S1[Ki[n]][, ref[n]], sum(y[n]));\n  }\n}",
    "crumbs": [
      "Data Analysis",
      "Data Analysis Model"
    ]
  },
  {
    "objectID": "Analyses/english.html",
    "href": "Analyses/english.html",
    "title": "English",
    "section": "",
    "text": "design = expand.grid(spos=c(0,1,2,3),\n            lpos=c(0,1,2,3)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(0,1,2,3),\n              word=c('este','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\nNow we load in the data.\n\neng = read.csv('Data/ENGLISH_DATA.csv')\n\nplt_data = eng %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_eng = eng %&gt;% select(-Coded_THIS) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter         mean     se_mean       2.5%          50%     97.5%     Rhat\n1  stau_lap -0.002774633 0.001773308 -0.1329530 -0.004102308 0.1334473 1.005678\n2  ltau_lap -0.003695181 0.002550365 -0.1868359 -0.002428074 0.1755724 1.005894\n  n_eff Bulk_ESS Tail_ESS\n1  1462     1468     1812\n2  1360     1367     1897\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 17.308, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9058519 0.9772217\nsample estimates:\n      cor \n0.9534001 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nNow that’s what these should be looking like!\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000,\n                )\n\n\n\nOur chains do not mix properly. Vanishing gradient :/ Not surprising. We didn’t think this would be a good model. Let’s put a hold on this for now.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%     97.5%     Rhat\n1  stau_lap -0.02190593 0.001453173 -0.1466488 -0.02365912 0.1068008 1.000989\n2  ltau_lap -0.06256301 0.002138258 -0.2473184 -0.06235000 0.1172175 1.000041\n  n_eff Bulk_ESS Tail_ESS\n1  1979     1989     2171\n2  1865     1875     2233\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 12.855, df = 30, p-value = 9.806e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8410931 0.9605374\nsample estimates:\n      cor \n0.9199735 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^4.98510730099115\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-4.98510730099115\n\n\n\n\n\nLet’s see for each subject, which semantics is a better fit.\n\neng %&gt;% select(-Coded_THIS) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    36\n2 Person      17",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#vizualize-the-data",
    "href": "Analyses/english.html#vizualize-the-data",
    "title": "English",
    "section": "",
    "text": "Now we load in the data.\n\neng = read.csv('Data/ENGLISH_DATA.csv')\n\nplt_data = eng %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_eng = eng %&gt;% select(-Coded_THIS) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#distance-semantics-model",
    "href": "Analyses/english.html#distance-semantics-model",
    "title": "English",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter         mean     se_mean       2.5%          50%     97.5%     Rhat\n1  stau_lap -0.002774633 0.001773308 -0.1329530 -0.004102308 0.1334473 1.005678\n2  ltau_lap -0.003695181 0.002550365 -0.1868359 -0.002428074 0.1755724 1.005894\n  n_eff Bulk_ESS Tail_ESS\n1  1462     1468     1812\n2  1360     1367     1897\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 17.308, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9058519 0.9772217\nsample estimates:\n      cor \n0.9534001 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nNow that’s what these should be looking like!",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#person-semantics-model",
    "href": "Analyses/english.html#person-semantics-model",
    "title": "English",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000,\n                )\n\n\n\nOur chains do not mix properly. Vanishing gradient :/ Not surprising. We didn’t think this would be a good model. Let’s put a hold on this for now.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%     97.5%     Rhat\n1  stau_lap -0.02190593 0.001453173 -0.1466488 -0.02365912 0.1068008 1.000989\n2  ltau_lap -0.06256301 0.002138258 -0.2473184 -0.06235000 0.1172175 1.000041\n  n_eff Bulk_ESS Tail_ESS\n1  1979     1989     2171\n2  1865     1875     2233\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 12.855, df = 30, p-value = 9.806e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8410931 0.9605374\nsample estimates:\n      cor \n0.9199735 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#model-comparison",
    "href": "Analyses/english.html#model-comparison",
    "title": "English",
    "section": "",
    "text": "Let’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^4.98510730099115\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-4.98510730099115",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#individual-subject-analysis",
    "href": "Analyses/english.html#individual-subject-analysis",
    "title": "English",
    "section": "",
    "text": "Let’s see for each subject, which semantics is a better fit.\n\neng %&gt;% select(-Coded_THIS) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    36\n2 Person      17",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#vizualize-the-data-1",
    "href": "Analyses/english.html#vizualize-the-data-1",
    "title": "English",
    "section": "Vizualize the Data",
    "text": "Vizualize the Data\n\neng = read.csv('Data/EXP1_DISTANCE_ENGLISH.csv')\n\nplt_data = eng %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_eng = eng %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                grepl('THAT', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\nWell this isn’t that different from the last experiment :/",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#person-semantics-model-1",
    "href": "Analyses/english.html#person-semantics-model-1",
    "title": "English",
    "section": "Person Semantics Model",
    "text": "Person Semantics Model\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_eng %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\nModel Checks\nLet’s check the chains\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nOkay so the gradient doesn’t always vanish but it’s not good. Must rethink the lower bound.\nThen the distributions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%      97.5%     Rhat\n1  stau_lap -0.06153285 0.001641665 -0.1836797 -0.06022668 0.06237886 1.001531\n2  ltau_lap -0.09564628 0.002415746 -0.2773152 -0.09335619 0.08545106 1.002033\n  n_eff Bulk_ESS Tail_ESS\n1  1483     1484     1920\n2  1420     1447     1884\n\n\nLastly, a face check.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nPlausible.\n\n\nPosterior Predictive Check\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 12.82, df = 30, p-value = 1.05e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8403671 0.9603454\nsample estimates:\n      cor \n0.9195921 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nThey’re both kinda good. Preliminary until I get the mixing working.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#model-comparison-1",
    "href": "Analyses/english.html#model-comparison-1",
    "title": "English",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^5.07196724209691\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-5.07196724209691\n\n\nDistance (as predicted)",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/english.html#individual-analysis",
    "href": "Analyses/english.html#individual-analysis",
    "title": "English",
    "section": "Individual Analysis",
    "text": "Individual Analysis\n\neng %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('THIS', Raw_Response) ~ 'este',\n                                grepl('THAT', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    34\n2 Person      17",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "SpatialDemos.html",
    "href": "SpatialDemos.html",
    "title": "Computational Model",
    "section": "",
    "text": "I made two minor changes:",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "SpatialDemos.html#literal-listener",
    "href": "SpatialDemos.html#literal-listener",
    "title": "Computational Model",
    "section": "Literal Listener",
    "text": "Literal Listener\nOur analysis model is a Rational Speech Act Model with a cost function determined by Incremental Collaborative Efficiency model. As a first step, we will need a literal Listener \\(\\mathcal{L}_0\\) that specifies the semantics. The overall form we will use is:\n\\[\\mathcal{L}_0(r|w) \\propto e^{A(r|w)/\\tau_L}\\]\nFor distance, \\[ A(r|ese) = -|r - spos|\\] \\[ A(r|ese) = 1 \\] \\[ A(r|aquel) = |r-spos| \\]\n\nsemantics_distance = design %&gt;%\n    mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                                grepl('ese', word) ~ 1,\n                                T ~ abs(referent - spos))) %&gt;%\n    group_by(Item, word) %&gt;%\n    mutate(L0 = softmax(scale(distance, scale = F)[,1])) %&gt;%\n    ungroup()\n\nFor person, \\[ A(r|ese) = -|r -spos| \\] \\[ A(r|ese) = |r-spos| - |r-lpos| \\] \\[ A(r|aquell) = |r-spos| + |r-lpos| \\]\n\nsemantics_person = design %&gt;%\n    mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                                grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                                T ~ abs(referent - spos) + abs(referent - lpos))) %&gt;%\n    group_by(Item, word) %&gt;%\n    mutate(L0 = softmax(scale(distance, scale = F)[,1])) %&gt;%\n    ungroup()",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "SpatialDemos.html#simple-speaker",
    "href": "SpatialDemos.html#simple-speaker",
    "title": "Computational Model",
    "section": "Simple Speaker",
    "text": "Simple Speaker\n\nICE Cost\nIn the CogSci paper, the ICE model expected visual search was computed via MonteCarlo simulations. To remove some stochasticity a bit, let’s instead marginalize over all possible fixation orders, which isn’t that bad as there are only 4 objects.\nIn the below function, I build up a square matrix look whose columns tell us the temporal order when we fixate something \\(fix_{[0:3]}\\) and the columns reflect which referent we are looking at for a given word.\n\\[P(fix_1 | r, w) = \\mathcal{L}_0(r|w)\\] \\[ P(fix_2 | r, w) = \\sum_{x \\neq r} \\mathcal{L}_0(x|w) \\frac{\\mathcal{L}_0(r|w)}{1 - \\mathcal{L}_0(x|w)} \\] \\[ P(fix_3 | r, w) = \\sum_{x \\neq r}\\sum_{y \\neq r} \\Biggr[ \\mathcal{L}_0(x|w) \\Biggr] \\Biggr[ \\frac{\\mathcal{L}_0(y|w)}{1 - \\mathcal{L}_0(y|w)} \\Biggr] \\Biggr[ \\frac{\\mathcal{L}_0(r|w)}{1 - \\mathcal{L}_0(x|w) - \\mathcal{L}_0(y|w)} \\Biggr] \\] \\[ P(fix_4 | r, w) = \\sum_{x \\neq r}\\sum_{y \\neq r}\\sum_{z \\neq r} \\Biggr[ \\mathcal{L}_0(x|w) \\Biggr] \\Biggr[ \\frac{\\mathcal{L}_0(y|w)}{1 - \\mathcal{L}_0(y|w)} \\Biggr] \\Biggr[ \\frac{\\mathcal{L}_0(z|w)}{1 - \\mathcal{L}_0(x|w) - \\mathcal{L}_0(y|w)} \\Biggr] \\] The expected cost is then: \\[ C(r|w) = 1 \\cdot P(fix_1|r, w) + 2\\cdot P(fix_2|r, w) + 3\\cdot P(fix_3|r,w) + 4\\cdot P(fix_4|r,w) \\]\nIn the function below, I use p to denote the fixation probability given by \\(\\mathcal{L}_0\\).\n\neye_cost = function(df){\n  df = df %&gt;% arrange(referent)\n  p = df$p\n  leng = length(p)\n  look = matrix(rep(0, leng*leng), leng, leng)\n  for(i in 0:3){\n    look[i+1, 1] = 1 * p[i+1]\n    pj = 1-p[i+1]\n    for(j in 0:3){\n      pk = pj-p[j+1]\n      if(j != i){\n        look[j+1, 2] = look[j+1, 2] + 2 * p[i+1] * p[j+1]/pj\n        for(k in 0:3){\n          pl = pk-p[k+1]\n          if(k != i & k != j){\n            look[k+1, 3] = look[k+1, 3] + 3 * p[i+1] * p[j+1]/pj * p[k+1]/pk\n            for (l in 0:3){\n              if(l != i & l != j & l != k){\n                look[l+1, 4] = look[l+1, 4] + 4 * p[i+1] * p[j+1]/pj * p[k+1]/pk * p[l+1]/pl\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  df %&gt;% mutate(cost=rowSums(look))\n}\n\n\nsemantics_distance %&lt;&gt;%\n    mutate(p=L0) %&gt;%\n    group_by(spos, lpos, word) %&gt;%\n    group_modify(~eye_cost(.))\n\nplot_cost_spos(semantics_distance)\n\n\n\n\n\n\n\nsemantics_person %&lt;&gt;%\n    mutate(p=L0) %&gt;%\n    group_by(spos, lpos, word) %&gt;%\n    group_modify(~eye_cost(.))\n\nplot_cost_spos(semantics_person)\n\n\n\n\n\n\n\n\nSo this looks right.\nThe simple speaker is then:\n\\[ \\mathcal{S}_0(w|r) \\propto e^{3-C(r|w)} \\], where we add 3 to center the cost.\n\nsemantics_distance %&lt;&gt;%\n    group_by(spos, lpos, referent) %&gt;%\n    arrange(spos, lpos, referent) %&gt;%\n    mutate(S0 = softmax((3 - cost))) %&gt;%\n    select(-cost, -p)\n\nsemantics_person %&lt;&gt;%\n    group_by(spos, lpos, referent) %&gt;%\n    arrange(spos, lpos, referent) %&gt;%\n    mutate(S0 = softmax((3 - cost))) %&gt;%\n    select(-cost, -p)",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "SpatialDemos.html#pragmatic-listener",
    "href": "SpatialDemos.html#pragmatic-listener",
    "title": "Computational Model",
    "section": "Pragmatic Listener",
    "text": "Pragmatic Listener\nSo usually the pragmatic listener is defined as: \\[ \\mathcal{L}_1(r|w) \\propto {\\mathcal{S}_0(w|r)} \\]. If we want to continue with the softmax and scale things then I would just log the simple speaker: \\[ \\mathcal{L}_1(r|w) \\propto e^{\\log \\mathcal{S}_0(w|r)}\\]\nThis is probably less pragmatically important but from an interpretation standpoint, the temperature parameter has a natural interpretation and scaling from probability theory.\n\nsemantics_distance %&lt;&gt;%\n  group_by(spos, lpos, word) %&gt;%\n  arrange(spos, lpos, word) %&gt;%\n  mutate(L1 = softmax(log(S0)))\n\nsemantics_person %&lt;&gt;%\n  group_by(spos, lpos, word) %&gt;%\n  arrange(spos, lpos, word) %&gt;%\n  mutate(L1 = softmax(log(S0)))",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "SpatialDemos.html#pragmatic-speaker",
    "href": "SpatialDemos.html#pragmatic-speaker",
    "title": "Computational Model",
    "section": "Pragmatic Speaker",
    "text": "Pragmatic Speaker\nLastly, \\[ \\mathcal{S}_1(w|r) \\propto e^{3-C(r|w)} \\]\n\nsemantics_distance %&lt;&gt;%\n    mutate(p=L1) %&gt;%\n  group_modify(~eye_cost(.)) %&gt;% \n  group_by(spos, lpos, referent) %&gt;%\n  arrange(spos, lpos, referent) %&gt;%\n  mutate(S1 = softmax((3 - cost))) %&gt;%\n  select(-cost, -p)\n\nplot_S1(semantics_distance)\n\n\n\n\n\n\n\nsemantics_person %&lt;&gt;%\n    mutate(p=L1) %&gt;%\n  group_modify(~eye_cost(.)) %&gt;% \n  group_by(spos, lpos, referent) %&gt;%\n  arrange(spos, lpos, referent) %&gt;%\n  mutate(S1 = softmax((3 - cost))) %&gt;%\n  select(-cost, -p)\n\nplot_S1(semantics_person)",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "SpatialDemos.html#putting-it-all-together",
    "href": "SpatialDemos.html#putting-it-all-together",
    "title": "Computational Model",
    "section": "Putting it all together",
    "text": "Putting it all together\nLet’s write a function that combines these steps and adds in the likelihood temperatures.\n\nrsa_model = function(df, ltau, stau){\n    df %&gt;%\n        # Literal Listener\n        group_by(Item, word) %&gt;%\n        mutate(L0 = softmax(scale(distance/ltau, scale = F)[,1])) %&gt;%\n        ungroup() %&gt;%\n        # ICE Cost\n        mutate(p=L0) %&gt;%\n        group_by(spos, lpos, word) %&gt;%\n        group_modify(~eye_cost(.)) %&gt;%\n        # Simple Speaker\n        group_by(spos, lpos, referent) %&gt;%\n        arrange(spos, lpos, referent) %&gt;%\n        mutate(S0 = softmax((3 - cost)/stau)) %&gt;%\n        select(-cost, -p) %&gt;%\n        # Pragmatic Listener\n        group_by(spos, lpos, word) %&gt;%\n        arrange(spos, lpos, word) %&gt;%\n        mutate(L1 = softmax(log(S0)/ltau)) %&gt;%\n        # ICE Cost\n        mutate(p=L1) %&gt;%\n        group_modify(~eye_cost(.)) %&gt;% \n        # Pragmatic Speaker\n        group_by(spos, lpos, referent) %&gt;%\n        arrange(spos, lpos, referent) %&gt;%\n        mutate(S1 = softmax((3 - cost)/stau)) %&gt;%\n        select(-cost, -p)\n}",
    "crumbs": [
      "Model",
      "Computational Model"
    ]
  },
  {
    "objectID": "Analyses/japanese.html",
    "href": "Analyses/japanese.html",
    "title": "Japanese",
    "section": "",
    "text": "design = expand.grid(spos=c(0,1,2,3),\n            lpos=c(0,1,2,3)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(0,1,2,3),\n              word=c('este','ese','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ 1,\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\n\njap = read.csv('Data/Japanese_DISTANCE.csv')\n\nplt_data = jap %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_jap = jap %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChains look good\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nAs does the distribution\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean      se_mean       2.5%        50%      97.5%      Rhat\n1  stau_lap -0.5165440 0.0009581848 -0.6183618 -0.5166308 -0.4098481 1.0007057\n2  ltau_lap -0.5750023 0.0017382251 -0.7441577 -0.5754524 -0.4001254 0.9999503\n  n_eff Bulk_ESS Tail_ESS\n1  3007     3011     2774\n2  2558     2610     2719\n\n\nand face validity?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_distance = plot_S1(S1_stan_dist)\nplt_distance\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_jap %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 9.0875, df = 46, p-value = 7.793e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6697871 0.8842018\nsample estimates:\n      cor \n0.8014063 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nHeavy are the chains\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nbut they mix. Distributions look\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.4773617 0.001346538 -0.5860596 -0.4766543 -0.37033449 1.001807\n2  ltau_lap -0.1965815 0.002238793 -0.3687474 -0.1976606 -0.02498487 1.002399\n  n_eff Bulk_ESS Tail_ESS\n1  1699     1715     1644\n2  1490     1506     1953\n\n\nnormal ;) and at face value\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_person = plot_S1(S1_stan_pers)\nplt_person\n\n\n\n\n\n\n\n\nthe predictions check out.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_jap %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 11.924, df = 46, p-value = 1.135e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7769808 0.9249218\nsample estimates:\n      cor \n0.8692202 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^-16.0030826659636\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^16.0030826659636\n\n\nSo distance is eeking out as slightly better but let’s take a look at the individual level.\n\n\n\n\njap %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                            grepl('A', Response) ~ 'este',\n                            grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    21\n2 Person      33\n\n\nIt’s a tie -_-",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/japanese.html#visualize-the-data",
    "href": "Analyses/japanese.html#visualize-the-data",
    "title": "Japanese",
    "section": "",
    "text": "jap = read.csv('Data/Japanese_DISTANCE.csv')\n\nplt_data = jap %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_jap = jap %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/japanese.html#distance-semantics-model",
    "href": "Analyses/japanese.html#distance-semantics-model",
    "title": "Japanese",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_jap %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChains look good\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nAs does the distribution\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean      se_mean       2.5%        50%      97.5%      Rhat\n1  stau_lap -0.5165440 0.0009581848 -0.6183618 -0.5166308 -0.4098481 1.0007057\n2  ltau_lap -0.5750023 0.0017382251 -0.7441577 -0.5754524 -0.4001254 0.9999503\n  n_eff Bulk_ESS Tail_ESS\n1  3007     3011     2774\n2  2558     2610     2719\n\n\nand face validity?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_distance = plot_S1(S1_stan_dist)\nplt_distance\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_jap %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 9.0875, df = 46, p-value = 7.793e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6697871 0.8842018\nsample estimates:\n      cor \n0.8014063 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/japanese.html#person-semantics-model",
    "href": "Analyses/japanese.html#person-semantics-model",
    "title": "Japanese",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_jap %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nHeavy are the chains\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nbut they mix. Distributions look\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.4773617 0.001346538 -0.5860596 -0.4766543 -0.37033449 1.001807\n2  ltau_lap -0.1965815 0.002238793 -0.3687474 -0.1976606 -0.02498487 1.002399\n  n_eff Bulk_ESS Tail_ESS\n1  1699     1715     1644\n2  1490     1506     1953\n\n\nnormal ;) and at face value\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_person = plot_S1(S1_stan_pers)\nplt_person\n\n\n\n\n\n\n\n\nthe predictions check out.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_jap %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 11.924, df = 46, p-value = 1.135e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7769808 0.9249218\nsample estimates:\n      cor \n0.8692202 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/japanese.html#model-comparison",
    "href": "Analyses/japanese.html#model-comparison",
    "title": "Japanese",
    "section": "",
    "text": "margin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^-16.0030826659636\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^16.0030826659636\n\n\nSo distance is eeking out as slightly better but let’s take a look at the individual level.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/japanese.html#individual-analysis",
    "href": "Analyses/japanese.html#individual-analysis",
    "title": "Japanese",
    "section": "",
    "text": "jap %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                            grepl('A', Response) ~ 'este',\n                            grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    21\n2 Person      33\n\n\nIt’s a tie -_-",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Japanese"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html",
    "href": "Analyses/portuguese.html",
    "title": "Portuguese",
    "section": "",
    "text": "design = expand.grid(spos=c(1,2,3,4),\n            lpos=c(1,2,3,4)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(1,2,3,4),\n              word=c('este','ese','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ 1,\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\nNow we load in the data.\n\npor = read.csv('Data/EXP1_DISTANCE_PORTUGUESE.csv')\n\nplt_data = por %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_por = por %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESS', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nFirst the chains\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nThen the distribution\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.4246206 0.001276267 -0.5349419 -0.4260279 -0.3116124 1.000560\n2  ltau_lap -0.2937050 0.002236337 -0.4752468 -0.2912855 -0.1213329 1.001926\n  n_eff Bulk_ESS Tail_ESS\n1  1998     2012     2418\n2  1664     1674     2266\n\n\nFinally the face.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nYep, this looks right.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_por %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 8.1451, df = 46, p-value = 1.809e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6196227 0.8639564\nsample estimates:\n      cor \n0.7684644 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nThis is horrible and that’s expected :)\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChains that wind\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nDistributional shape?\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5343092 0.001306373 -0.6401655 -0.5345664 -0.4199504 1.001711\n2  ltau_lap -0.2088843 0.002040674 -0.3835621 -0.2081466 -0.0423049 1.001005\n  n_eff Bulk_ESS Tail_ESS\n1  1874     1888     2138\n2  1795     1815     1925\n\n\nFace validity?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\nplt_pers\n\n\n\n\n\n\n\n\nYeah that’s what’s expected.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_por %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 15.792, df = 46, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8589684 0.9539324\nsample estimates:\n      cor \n0.9188468 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nYay that’s a somewhat okay fit.\n\n\n\n\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^-38.8445354526558\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^38.8445354526558\n\n\nNot surprising that person is the better model here.\n\n\n\n\npor %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESS', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    11\n2 Person      39\n\n\nAnd most of the participants are best fit by person, althought a decent number are still best explained by distance.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html#vizualize-the-data",
    "href": "Analyses/portuguese.html#vizualize-the-data",
    "title": "Portuguese",
    "section": "",
    "text": "Now we load in the data.\n\npor = read.csv('Data/EXP1_DISTANCE_PORTUGUESE.csv')\n\nplt_data = por %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_por = por %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESS', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html#distance-semantics-model",
    "href": "Analyses/portuguese.html#distance-semantics-model",
    "title": "Portuguese",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_por %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nFirst the chains\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nThen the distribution\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.4246206 0.001276267 -0.5349419 -0.4260279 -0.3116124 1.000560\n2  ltau_lap -0.2937050 0.002236337 -0.4752468 -0.2912855 -0.1213329 1.001926\n  n_eff Bulk_ESS Tail_ESS\n1  1998     2012     2418\n2  1664     1674     2266\n\n\nFinally the face.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nYep, this looks right.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_por %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 8.1451, df = 46, p-value = 1.809e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6196227 0.8639564\nsample estimates:\n      cor \n0.7684644 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nThis is horrible and that’s expected :)",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html#model-comparison",
    "href": "Analyses/portuguese.html#model-comparison",
    "title": "Portuguese",
    "section": "",
    "text": "margin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^-38.8445354526558\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^38.8445354526558\n\n\nNot surprising that person is the better model here.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html#individual-analysis",
    "href": "Analyses/portuguese.html#individual-analysis",
    "title": "Portuguese",
    "section": "",
    "text": "por %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESS', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    11\n2 Person      39\n\n\nAnd most of the participants are best fit by person, althought a decent number are still best explained by distance.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "Analyses/spanish.html",
    "href": "Analyses/spanish.html",
    "title": "Spanish",
    "section": "",
    "text": "Here is the design and the literal listeners.\n\ndesign = expand.grid(spos=c(0,1,2,3),\n            lpos=c(0,1,2,3)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(0,1,2,3),\n              word=c('este','ese','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ 1,\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\nNow we load in the data.\n\nspa = read.csv('Data/SPANISH_DATA.csv')\n\nplt_data = spa %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_spa = spa %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('ESE', Response) ~ 'ese',\n                                grepl('ESTE', Response) ~ 'este',\n                                grepl('AQU', Response) ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5603747 0.001086543 -0.6720244 -0.5603758 -0.4498211 1.000201\n2  ltau_lap -0.6031640 0.001760478 -0.7823580 -0.6022204 -0.4207983 1.002156\n  n_eff Bulk_ESS Tail_ESS\n1  2698     2731     2658\n2  2745     2772     2488\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nThe facet rows reflect the referent. The pragmatic speaker distribution should be compared across cells of the columns. This looks good to me.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 10.704, df = 46, p-value = 4.471e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7376147 0.9103520\nsample estimates:\n      cor \n0.8447129 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nx-axis is empirical and y-axis is model prediction. Not the greatest fit but not bad.\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%        97.5%     Rhat\n1  stau_lap -0.3648614 0.001358030 -0.4731119 -0.3653027 -0.250966902 1.001030\n2  ltau_lap -0.1723046 0.002209665 -0.3481063 -0.1703499  0.002158329 1.001282\n  n_eff Bulk_ESS Tail_ESS\n1  1750     1748     2152\n2  1598     1612     1755\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nAgain seems plausible to me.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 9.4128, df = 46, p-value = 2.69e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6851286 0.8902365\nsample estimates:\n      cor \n0.8113238 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nAgain, not the greatest looking fits.\n\n\n\n\nLet’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^14.0949807427932\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-14.0949807427932\n\n\nSo this analysis actually suggests that Spanish is better explained by a distance semantics.\n\n\n\nLet’s see for each subject, which semantics is a better fit.\n\nspa %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('ESE', Response) ~ 'ese',\n                                grepl('ESTE', Response) ~ 'este',\n                                grepl('AQU', Response) ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Participant) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    34\n2 Person      17\n\n\nSo while there are clearly some people who are better described as using person semantics, it looks like the majority of the sample are better explained by distance. Let’s look at the other spanish dataset.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#vizualize-the-data",
    "href": "Analyses/spanish.html#vizualize-the-data",
    "title": "Spanish",
    "section": "",
    "text": "Now we load in the data.\n\nspa = read.csv('Data/SPANISH_DATA.csv')\n\nplt_data = spa %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_spa = spa %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('ESE', Response) ~ 'ese',\n                                grepl('ESTE', Response) ~ 'este',\n                                grepl('AQU', Response) ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#distance-semantics-model",
    "href": "Analyses/spanish.html#distance-semantics-model",
    "title": "Spanish",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5603747 0.001086543 -0.6720244 -0.5603758 -0.4498211 1.000201\n2  ltau_lap -0.6031640 0.001760478 -0.7823580 -0.6022204 -0.4207983 1.002156\n  n_eff Bulk_ESS Tail_ESS\n1  2698     2731     2658\n2  2745     2772     2488\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nThe facet rows reflect the referent. The pragmatic speaker distribution should be compared across cells of the columns. This looks good to me.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 10.704, df = 46, p-value = 4.471e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7376147 0.9103520\nsample estimates:\n      cor \n0.8447129 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nx-axis is empirical and y-axis is model prediction. Not the greatest fit but not bad.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#person-semantics-model",
    "href": "Analyses/spanish.html#person-semantics-model",
    "title": "Spanish",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%        97.5%     Rhat\n1  stau_lap -0.3648614 0.001358030 -0.4731119 -0.3653027 -0.250966902 1.001030\n2  ltau_lap -0.1723046 0.002209665 -0.3481063 -0.1703499  0.002158329 1.001282\n  n_eff Bulk_ESS Tail_ESS\n1  1750     1748     2152\n2  1598     1612     1755\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nAgain seems plausible to me.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 9.4128, df = 46, p-value = 2.69e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6851286 0.8902365\nsample estimates:\n      cor \n0.8113238 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nAgain, not the greatest looking fits.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#model-comparison",
    "href": "Analyses/spanish.html#model-comparison",
    "title": "Spanish",
    "section": "",
    "text": "Let’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^14.0949807427932\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-14.0949807427932\n\n\nSo this analysis actually suggests that Spanish is better explained by a distance semantics.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#individual-subject-analysis",
    "href": "Analyses/spanish.html#individual-subject-analysis",
    "title": "Spanish",
    "section": "",
    "text": "Let’s see for each subject, which semantics is a better fit.\n\nspa %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('ESE', Response) ~ 'ese',\n                                grepl('ESTE', Response) ~ 'este',\n                                grepl('AQU', Response) ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Participant) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    34\n2 Person      17\n\n\nSo while there are clearly some people who are better described as using person semantics, it looks like the majority of the sample are better explained by distance. Let’s look at the other spanish dataset.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#vizualize-the-data-1",
    "href": "Analyses/spanish.html#vizualize-the-data-1",
    "title": "Spanish",
    "section": "Vizualize the Data",
    "text": "Vizualize the Data\n\nspa = read.csv('Data/EXP1_DISTANCE_SPANISH.csv')\n\nplt_data = spa %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_spa = spa %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESE', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\nWell this isn’t that different from the last experiment :/",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#person-semantics-model-1",
    "href": "Analyses/spanish.html#person-semantics-model-1",
    "title": "Spanish",
    "section": "Person Semantics Model",
    "text": "Person Semantics Model\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_spa %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\nModel Checks\nLet’s check the chains\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.4340153 0.001313959 -0.5370746 -0.4348940 -0.32625342 1.001008\n2  ltau_lap -0.2268845 0.002123616 -0.3842573 -0.2252731 -0.06458559 1.001296\n  n_eff Bulk_ESS Tail_ESS\n1  1691     1739     2422\n2  1511     1509     2123\n\n\nLastly, a face check.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nPassable.\n\n\nPosterior Predictive Check\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 9.6399, df = 46, p-value = 1.289e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6952994 0.8941980\nsample estimates:\n     cor \n0.817859 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nYeah neither of these are suuper nice (but I am a stickler for perfection).",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#model-comparison-1",
    "href": "Analyses/spanish.html#model-comparison-1",
    "title": "Spanish",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^10.446497473884\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-10.446497473884\n\n\nSo again Distance based semantics seems the better fit. But what if we look at individuals.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#individual-analysis",
    "href": "Analyses/spanish.html#individual-analysis",
    "title": "Spanish",
    "section": "Individual Analysis",
    "text": "Individual Analysis\n\nspa %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('ESE', Raw_Response) ~ 'ese',\n                                grepl('ESTE', Raw_Response) ~ 'este',\n                                grepl('AQU', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    30\n2 Person      20\n\n\nAnd this is a very similar ratio across participants.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html",
    "href": "Analyses/turkish.html",
    "title": "Turkish",
    "section": "",
    "text": "design = expand.grid(spos=c(0,1,2,3),\n            lpos=c(0,1,2,3)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(0,1,2,3),\n              word=c('este','ese','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ 1,\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\n\ntur = read.csv('Data/Turkish_DISTANCE.csv')\n\nplt_data = tur %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_tur = tur %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nFirst the chains\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nthen the distributions\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.2696526 0.001174398 -0.3853310 -0.2697048 -0.15148914 1.001196\n2  ltau_lap -0.2956359 0.001965949 -0.4911046 -0.2959271 -0.09511482 1.001163\n  n_eff Bulk_ESS Tail_ESS\n1  2542     2563     2159\n2  2656     2676     2919\n\n\nLast a face validity check\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_distance = plot_S1(S1_stan_dist)\nplt_distance\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_tur %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 8.0259, df = 46, p-value = 2.709e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6126154 0.8610642\nsample estimates:\n      cor \n0.7637992 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nThat’s not bad at all.\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChain, chain, chain\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nDistributions\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%      97.5%     Rhat\n1  stau_lap -0.03713073 0.001432559 -0.1544918 -0.03670790 0.07693479 1.002556\n2  ltau_lap -0.09153613 0.002331499 -0.2729750 -0.09086176 0.08648473 1.001733\n  n_eff Bulk_ESS Tail_ESS\n1  1724     1777     1902\n2  1525     1567     1899\n\n\nFinally, face validity.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_tur %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 5.9895, df = 46, p-value = 3.002e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4653244 0.7963060\nsample estimates:\n      cor \n0.6619408 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nDistance looks better.\n\n\n\n\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^23.9614182376444\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-23.9614182376444\n\n\n\n\n\n\ntur %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    37\n2 Person      14",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html#vizualize-the-data",
    "href": "Analyses/turkish.html#vizualize-the-data",
    "title": "Turkish",
    "section": "",
    "text": "tur = read.csv('Data/Turkish_DISTANCE.csv')\n\nplt_data = tur %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_tur = tur %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html#distance-semantics-model",
    "href": "Analyses/turkish.html#distance-semantics-model",
    "title": "Turkish",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_tur %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nFirst the chains\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nthen the distributions\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.2696526 0.001174398 -0.3853310 -0.2697048 -0.15148914 1.001196\n2  ltau_lap -0.2956359 0.001965949 -0.4911046 -0.2959271 -0.09511482 1.001163\n  n_eff Bulk_ESS Tail_ESS\n1  2542     2563     2159\n2  2656     2676     2919\n\n\nLast a face validity check\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_distance = plot_S1(S1_stan_dist)\nplt_distance\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_tur %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 8.0259, df = 46, p-value = 2.709e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6126154 0.8610642\nsample estimates:\n      cor \n0.7637992 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nThat’s not bad at all.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html#person-semantics-model",
    "href": "Analyses/turkish.html#person-semantics-model",
    "title": "Turkish",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_tur %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChain, chain, chain\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nDistributions\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%      97.5%     Rhat\n1  stau_lap -0.03713073 0.001432559 -0.1544918 -0.03670790 0.07693479 1.002556\n2  ltau_lap -0.09153613 0.002331499 -0.2729750 -0.09086176 0.08648473 1.001733\n  n_eff Bulk_ESS Tail_ESS\n1  1724     1777     1902\n2  1525     1567     1899\n\n\nFinally, face validity.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este'),\n           referent = referent-1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_tur %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 5.9895, df = 46, p-value = 3.002e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4653244 0.7963060\nsample estimates:\n      cor \n0.6619408 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nDistance looks better.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html#model-comparison",
    "href": "Analyses/turkish.html#model-comparison",
    "title": "Turkish",
    "section": "",
    "text": "margin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^23.9614182376444\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-23.9614182376444",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/turkish.html#individual-analysis",
    "href": "Analyses/turkish.html#individual-analysis",
    "title": "Turkish",
    "section": "",
    "text": "tur %&gt;% select(-Coded_ESTE, -Coded_ESE, -Coded_AQUEL) %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('B', Response) ~ 'ese',\n                                grepl('A', Response) ~ 'este',\n                                grepl('C', Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    37\n2 Person      14",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Turkish"
    ]
  },
  {
    "objectID": "Analyses/italian.html",
    "href": "Analyses/italian.html",
    "title": "Italian",
    "section": "",
    "text": "ita = read.csv('Data/EXP1_DISTANCE_ITALIAN.csv') %&gt;%\n    filter(!is.na(Object_Position))\n\nplt_data = ita %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_ita = ita %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('EST', Raw_Response) ~ 'este',\n                                grepl('ELL', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains converge\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributional assumptions look right.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.4949014 0.001476379 -0.6170489 -0.4966669 -0.36784139 1.002954\n2  ltau_lap -0.2141260 0.002033371 -0.3915056 -0.2117226 -0.04469728 1.001324\n  n_eff Bulk_ESS Tail_ESS\n1  1840     1867     2280\n2  1836     1862     2346\n\n\nAnd now a face validity check.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nAgain looks pretty on target.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ita %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 23.222, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9454768 0.9870133\nsample estimates:\n      cor \n0.9732934 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nYeah this looks spot on.\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s check the chains\nYeah this inference needs different bounds.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5314097 0.001310479 -0.6502717 -0.5328652 -0.4119787 1.000150\n2  ltau_lap -0.3192702 0.001944213 -0.4935620 -0.3182817 -0.1511265 1.000802\n  n_eff Bulk_ESS Tail_ESS\n1  2177     2181     2119\n2  2014     2026     2081\n\n\nLastly, a face check.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nPassable.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ita %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 16.878, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9014585 0.9761170\nsample estimates:\n      cor \n0.9511687 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^8.82920320802557\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-8.82920320802557\n\n\nSo again Distance based semantics seems the better fit. But what if we look at individuals.\n\n\n\n\nita %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('EST', Raw_Response) ~ 'este',\n                                grepl('ELL', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    42\n2 Person       9\n\n\nAnd this is a very similar ratio across participants.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/italian.html#vizualize-the-data",
    "href": "Analyses/italian.html#vizualize-the-data",
    "title": "Italian",
    "section": "",
    "text": "ita = read.csv('Data/EXP1_DISTANCE_ITALIAN.csv') %&gt;%\n    filter(!is.na(Object_Position))\n\nplt_data = ita %&gt;%\n  group_by(Object_Position, Listener_Position, Raw_Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Raw_Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_ita = ita %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('EST', Raw_Response) ~ 'este',\n                                grepl('ELL', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/italian.html#person-semantics-model",
    "href": "Analyses/italian.html#person-semantics-model",
    "title": "Italian",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_ita %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s check the chains\nYeah this inference needs different bounds.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributions.\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5314097 0.001310479 -0.6502717 -0.5328652 -0.4119787 1.000150\n2  ltau_lap -0.3192702 0.001944213 -0.4935620 -0.3182817 -0.1511265 1.000802\n  n_eff Bulk_ESS Tail_ESS\n1  2177     2181     2119\n2  2014     2026     2081\n\n\nLastly, a face check.\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\nPassable.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ita %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 16.878, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9014585 0.9761170\nsample estimates:\n      cor \n0.9511687 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/italian.html#model-comparison",
    "href": "Analyses/italian.html#model-comparison",
    "title": "Italian",
    "section": "",
    "text": "margin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^8.82920320802557\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-8.82920320802557\n\n\nSo again Distance based semantics seems the better fit. But what if we look at individuals.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/italian.html#individual-analysis",
    "href": "Analyses/italian.html#individual-analysis",
    "title": "Italian",
    "section": "",
    "text": "ita %&gt;% \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 1,\n           word = case_when(grepl('EST', Raw_Response) ~ 'este',\n                                grepl('ELL', Raw_Response) ~ 'aquel')) %&gt;%\n    select(Subject, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Subject) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    42\n2 Person       9\n\n\nAnd this is a very similar ratio across participants.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/catalan.html",
    "href": "Analyses/catalan.html",
    "title": "Catalan",
    "section": "",
    "text": "design = expand.grid(spos=c(0,1,2,3),\n            lpos=c(0,1,2,3)) %&gt;%\n  mutate(Item=1:n()) %&gt;%\n  expand_grid(referent=c(0,1,2,3),\n              word=c('este','aquel')) %&gt;%\n  select(Item, spos, lpos, word, referent) \n\nW = design %&gt;% pull(word) %&gt;% unique() %&gt;% length\nR = design %&gt;% pull(referent) %&gt;% unique() %&gt;% length\nK = design %&gt;% pull(Item) %&gt;% unique() %&gt;% length\n\nsemantics_distance = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n  mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                              T ~ abs(referent - spos) + abs(referent - lpos)))\n\n\n\nNow we load in the data.\n\nctn = read.csv('Data/Catalan_DISTANCE.csv') %&gt;%\n    rename(Object_Position=Object_position,\n           Listener_Position=Listener_position)\n\nplt_data = ctn %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_ctn = ctn %&gt;%  \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('EST', Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`\n\n\n\n\n\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.5689835 0.001275943 -0.6916035 -0.5694201 -0.44655422 1.000868\n2  ltau_lap -0.2621491 0.001801470 -0.4270309 -0.2625608 -0.09332263 1.001958\n  n_eff Bulk_ESS Tail_ESS\n1  2299     2316     2404\n2  2236     2245     2371\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ctn %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 37.352, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9782131 0.9948763\nsample estimates:\n      cor \n0.9894191 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nNow that’s what these should be looking like!\n\n\n\n\n\nstan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67652,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nThat mixed very well.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.6502235 0.001367368 -0.7700506 -0.6504501 -0.5311778 1.000490\n2  ltau_lap -0.3061831 0.001807230 -0.4801341 -0.3049682 -0.1423603 1.000471\n  n_eff Bulk_ESS Tail_ESS\n1  2069     2092     2399\n2  2171     2180     2139\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ctn %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 33.801, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9735200 0.9937613\nsample estimates:\n      cor \n0.9871242 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^2.33520919762411\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-2.33520919762411\n\n\n\n\n\nLet’s see for each subject, which semantics is a better fit.\n\nctn %&gt;%  \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('EST', Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Participant) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    37\n2 Person      15\n\n\nOhh that’s an odd one, the person model looked so good :/",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "Analyses/catalan.html#vizualize-the-data",
    "href": "Analyses/catalan.html#vizualize-the-data",
    "title": "Catalan",
    "section": "",
    "text": "Now we load in the data.\n\nctn = read.csv('Data/Catalan_DISTANCE.csv') %&gt;%\n    rename(Object_Position=Object_position,\n           Listener_Position=Listener_position)\n\nplt_data = ctn %&gt;%\n  group_by(Object_Position, Listener_Position, Response) %&gt;%\n  summarise(N=n()) %&gt;%\n  group_by(Object_Position, Listener_Position) %&gt;%\n  mutate(p = N / sum(N)) %&gt;%\n  ggplot(aes(Listener_Position, Object_Position)) +\n    facet_wrap(~Response) +\n    geom_tile(aes(fill=p)) +\n    scale_fill_steps2(limits=c(0, 1), n.breaks=10) +\n    ylab('Referent Position') +\n    xlab('Listener Position') +\n    ggtitle('Empirical S1') +\n    guides(fill='none') +\n    theme_bw(base_size = 12) \n\n`summarise()` has grouped output by 'Object_Position', 'Listener_Position'. You\ncan override using the `.groups` argument.\n\nplt_data\n\n\n\n\n\n\n\nd_ctn = ctn %&gt;%  \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('EST', Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    group_by(spos, lpos, word, referent) %&gt;%\n    summarise(y = n()) %&gt;%\n    left_join(design) %&gt;%\n    drop_na() %&gt;%\n    group_by(Item, referent) %&gt;%\n    mutate(N=sum(y)) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'spos', 'lpos', 'word'. You can override\nusing the `.groups` argument.\nJoining with `by = join_by(spos, lpos, word, referent)`",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "Analyses/catalan.html#distance-semantics-model",
    "href": "Analyses/catalan.html#distance-semantics-model",
    "title": "Catalan",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              mutate(referent = referent+1) %&gt;%\n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_ctn %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains mixed properly\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand our distributional assumptions.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.5689835 0.001275943 -0.6916035 -0.5694201 -0.44655422 1.000868\n2  ltau_lap -0.2621491 0.001801470 -0.4270309 -0.2625608 -0.09332263 1.001958\n  n_eff Bulk_ESS Tail_ESS\n1  2299     2316     2404\n2  2236     2245     2371\n\n\nSo what do our predictions look like?\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ctn %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 37.352, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9782131 0.9948763\nsample estimates:\n      cor \n0.9894191 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nNow that’s what these should be looking like!",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "Analyses/catalan.html#person-semantics-model",
    "href": "Analyses/catalan.html#person-semantics-model",
    "title": "Catalan",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               mutate(referent = referent+1) %&gt;%\n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_ctn %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67652,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nThat mixed very well.\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.6502235 0.001367368 -0.7700506 -0.6504501 -0.5311778 1.000490\n2  ltau_lap -0.3061831 0.001807230 -0.4801341 -0.3049682 -0.1423603 1.000471\n  n_eff Bulk_ESS Tail_ESS\n1  2069     2092     2399\n2  2171     2180     2139\n\n\nAnd do we pass the face valid test?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este'),\n           referent = referent - 1) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\n\nplt_pers\n\n\n\n\n\n\n\n\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ctn %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 33.801, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9735200 0.9937613\nsample estimates:\n      cor \n0.9871242 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "Analyses/catalan.html#model-comparison",
    "href": "Analyses/catalan.html#model-comparison",
    "title": "Catalan",
    "section": "",
    "text": "Let’s compute Bayes Factors, marginalizing over the paremeters using bridge sampling.\n\nmargin_dist = bridge_sampler(fit_dist)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\n\nmargin_pers = bridge_sampler(fit_pers)\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\n\n# Dist &gt; Person\nmessage('Dist &gt; Person: 10^', log10(exp(margin_dist$logml - margin_pers$logml)))\n\nDist &gt; Person: 10^2.33520919762411\n\n# Person &gt; Dist\nmessage('Person &gt; Dist: 10^', log10(exp(margin_pers$logml - margin_dist$logml)))\n\nPerson &gt; Dist: 10^-2.33520919762411",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "Analyses/catalan.html#individual-subject-analysis",
    "href": "Analyses/catalan.html#individual-subject-analysis",
    "title": "Catalan",
    "section": "",
    "text": "Let’s see for each subject, which semantics is a better fit.\n\nctn %&gt;%  \n    rename(Trial=Item, referent=Object_Position, lpos=Listener_Position) %&gt;%\n    mutate(spos = 0,\n           word = case_when(grepl('EST', Response) ~ 'este',\n                                T ~ 'aquel')) %&gt;%\n    select(Participant, Trial, spos, lpos, referent, word) %&gt;%\n    left_join(S1_stan_dist %&gt;% rename(dist=S1)) %&gt;%\n    left_join(S1_stan_pers %&gt;% rename(pers=S1)) %&gt;%\n    group_by(Participant) %&gt;%\n    summarise(like_dist = sum(log(dist)),\n              like_pers = sum(log(pers))) %&gt;%\n    mutate(d_factor = like_dist - like_pers,\n           Model = case_when(d_factor &gt; 0 ~ 'Distance',\n                             d_factor &lt; 0 ~ 'Person',\n                             T ~ NA)) %&gt;% \n    group_by(Model) %&gt;%\n    summarise(N=n())\n\nJoining with `by = join_by(spos, lpos, referent, word)`\nJoining with `by = join_by(spos, lpos, referent, word, Item)`\n\n\n# A tibble: 2 × 2\n  Model        N\n  &lt;chr&gt;    &lt;int&gt;\n1 Distance    37\n2 Person      15\n\n\nOhh that’s an odd one, the person model looked so good :/",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Catalan"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Demonstratives",
    "section": "",
    "text": "Hello friends,\nI had nothing better to do over the holidays this year so I took a stab at building a data analysis model based on your code. I started off by just comparing distance vs person based semantics for the datasets that Paula had shared with me. My results, summarised below, are consistent with the predictions shared with me, with one notable exception: Spanish.\nTo start, I made two minor changes to the model in turning it into a data analysis model. I’m not sure how they would cause Spanish to flip though. - Calculated cost from the ICE model analytically by marginalizing over fixation order. - Log the literal simple speaker term in the literal listener equation.\nThe model changes are described here : Model.\nThe data analyses can be found here: Data Analysis.\nBelow I’ll plot the inferred parameters and the group level and individual level classification results."
  },
  {
    "objectID": "TempViz.html",
    "href": "TempViz.html",
    "title": "Exploring Temperatures",
    "section": "",
    "text": "Let’s plot a baseline without temperature scaling.\n\ndesign = expand.grid(spos=c(1,2,3,4),\n                     lpos=c(1,2,3,4)) %&gt;%\n    mutate(Item=1:n()) %&gt;%  # Let's give each spos/lpos combo a unique Item number reflecting the task context\n    expand_grid(referent=c(1,2,3,4),\n                word=c('este','ese','aquel')) %&gt;%\n    select(Item, spos, lpos, word, referent)\n\nsemantics_distance = design %&gt;%\n    mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                                grepl('ese', word) ~ 1,\n                                T ~ abs(referent - spos)))\n\nsemantics_person = design %&gt;%\n    mutate(distance = case_when(grepl('este', word) ~ -abs(referent - spos),\n                                grepl('ese', word) ~ abs(referent - spos) - abs(referent - lpos),\n                                T ~ abs(referent - spos) + abs(referent - lpos)))\n\ndistance_1_1 = rsa_model(semantics_distance, 1., 1.)\n\nperson_1_1 = rsa_model(semantics_person, 1., 1.)\n\nplot_S1(distance_1_1) + plot_S1(person_1_1)\n\n\n\n\n\n\n\n\n\nListener Temperature\nThe listener temperature should fine tune the literal listener. I\n\ndistance_n2_1 = rsa_model(semantics_distance, 0.5, 1.)\n\nperson_n2_1 = rsa_model(semantics_person, 0.5, 1.)\n\nplot_S1(distance_1_1) + plot_S1(distance_n2_1)\n\n\n\n\n\n\n\n\nAquel and este become more back and white and ese gains shades of grey through pragmatic inference.\n\nplot_S1(person_1_1) + plot_S1(person_n2_1)\n\n\n\n\n\n\n\n\nSimilarly here.\nNow to demonstrate the other way.\n\ndistance_2_1 = rsa_model(semantics_distance, 2, 1.)\n\nperson_2_1 = rsa_model(semantics_person, 2, 1.)\n\nplot_S1(distance_1_1) + plot_S1(distance_2_1)\n\n\n\n\n\n\n\nplot_S1(person_1_1) + plot_S1(person_2_1)\n\n\n\n\n\n\n\n\nAs expected, everything gets washed out. It’s normally unlikely we see a temperature greater than 1 and if we do it’s normally very small and suggests noise.\n\n\nSpeaker Temperature\n\ndistance_1_n2 = rsa_model(semantics_distance, 1, 0.5)\n\nperson_1_n2 = rsa_model(semantics_person, 1, 0.5)\n\nplot_S1(distance_1_1) + plot_S1(distance_1_n2)\n\n\n\n\n\n\n\nplot_S1(person_1_1) + plot_S1(person_1_n2)\n\n\n\n\n\n\n\n\nAgain the predictions sharpen, although more rigidly than increasing the listener temperature. Compare the left plot with the increased listener temp and the right most plot with the incerased speaker temperature.\n\nplot_S1(distance_n2_1) + plot_S1(distance_1_1) + plot_S1(distance_1_n2)\n\n\n\n\n\n\n\nplot_S1(person_n2_1) + plot_S1(person_1_1) + plot_S1(person_1_n2)\n\n\n\n\n\n\n\n\nAnd now if we relax the speaker temperature.\n\ndistance_1_2 = rsa_model(semantics_distance, 1, 2.)\n\nperson_1_2 = rsa_model(semantics_person, 1, 2.)\n\nplot_S1(distance_1_1) + plot_S1(distance_1_2)\n\n\n\n\n\n\n\nplot_S1(person_1_1) + plot_S1(person_1_2)\n\n\n\n\n\n\n\n\nAgain, it washes out. So we shouldn’t expect temperatures to exceed one.\n\n\nMixing it up\n\ndistance_n3_a1 = rsa_model(semantics_distance, 0.125, 1.2)\n\nperson_n3_a1 = rsa_model(semantics_person, 0.125, 1.2)\n\nplot_S1(distance_1_1) + plot_S1(distance_n2_1) + plot_S1(distance_n3_a1)\n\n\n\n\n\n\n\nplot_S1(person_1_1) + plot_S1(person_n2_1) + plot_S1(person_n3_a1)",
    "crumbs": [
      "Model",
      "Exploring Temperatures"
    ]
  },
  {
    "objectID": "Analyses/english.html#distance-semantics-model-1",
    "href": "Analyses/english.html#distance-semantics-model-1",
    "title": "English",
    "section": "Distance Semantics Model",
    "text": "Distance Semantics Model\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_eng %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\nModel Checks\nLet’s make sure the chains converge\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributional assumptions look right.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter        mean     se_mean       2.5%         50%      97.5%     Rhat\n1  stau_lap -0.04075209 0.001580751 -0.1643603 -0.04242292 0.08570836 1.001424\n2  ltau_lap -0.02769374 0.002225273 -0.2010720 -0.02754335 0.14295344 1.001728\n  n_eff Bulk_ESS Tail_ESS\n1  1605     1611     2361\n2  1523     1533     2020\n\n\nAnd now a face validity check.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nAgain looks pretty on target.\n\n\nPosterior Predictive Check\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_eng %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 17.19, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9046728 0.9769256\nsample estimates:\n      cor \n0.9528017 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nYeah this looks spot on.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "English"
    ]
  },
  {
    "objectID": "Analyses/spanish.html#distance-semantics-model-1",
    "href": "Analyses/spanish.html#distance-semantics-model-1",
    "title": "Spanish",
    "section": "Distance Semantics Model",
    "text": "Distance Semantics Model\n\nstan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_spa %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, ese, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\nModel Checks\nLet’s make sure the chains converge\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributional assumptions look right.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%      Rhat\n1  stau_lap -0.6547333 0.001012311 -0.7666563 -0.6548407 -0.5505723 0.9999839\n2  ltau_lap -0.7271031 0.001580273 -0.8885231 -0.7277707 -0.5627563 0.9998966\n  n_eff Bulk_ESS Tail_ESS\n1  2934     2950     2466\n2  2725     2731     2505\n\n\nAnd now a face validity check.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Check\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_spa %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 10.811, df = 46, p-value = 3.213e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7414314 0.9117835\nsample estimates:\n      cor \n0.8471087 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Spanish"
    ]
  },
  {
    "objectID": "Analyses/italian.html#distance-semantics-model",
    "href": "Analyses/italian.html#distance-semantics-model",
    "title": "Italian",
    "section": "",
    "text": "stan_data_distance = list(W = W,\n                          R = R,\n                          K = K,\n                          distance = semantics_distance %&gt;% \n                              arrange(word, referent, Item) %&gt;% \n                              pull(distance) %&gt;% \n                              array(dim=c(K, R, W)),\n                          N_resp = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              nrow(),\n                          Ki = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(Item) %&gt;% as.integer(),\n                          ref = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;% \n                              pull(referent) %&gt;% \n                              as.integer(),\n                          y = d_ita %&gt;%\n                              arrange(Item) %&gt;%\n                              spread(word, y, fill = 0) %&gt;%\n                              select(aquel, este) %&gt;%\n                              as.matrix() %&gt;%\n                              unname()\n)\n\nfit_dist = stan('model.stan',\n                data=stan_data_distance,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nLet’s make sure the chains converge\n\nfit_dist %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nand the distributional assumptions look right.\n\nfit_dist %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_dist, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%       97.5%     Rhat\n1  stau_lap -0.4949014 0.001476379 -0.6170489 -0.4966669 -0.36784139 1.002954\n2  ltau_lap -0.2141260 0.002033371 -0.3915056 -0.2117226 -0.04469728 1.001324\n  n_eff Bulk_ESS Tail_ESS\n1  1840     1867     2280\n2  1836     1862     2346\n\n\nAnd now a face validity check.\n\nS1_stan_dist = fit_dist %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_dist = plot_S1(S1_stan_dist)\nplt_dist\n\n\n\n\n\n\n\n\nAgain looks pretty on target.\n\n\n\n\ny_stan_distance = fit_dist %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(distance=mean(y_rep), \n              distance_l=quantile(y_rep, 0.025),\n              distance_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            T ~ 'este')) %&gt;%\n    left_join(d_ita %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_distance %&gt;%\n    with(cor.test(distance, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  distance and y\nt = 23.222, df = 30, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9454768 0.9870133\nsample estimates:\n      cor \n0.9732934 \n\nplot_model_distance = y_stan_distance %&gt;%\n    ggplot(aes(y, distance)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=distance_l, ymax=distance_h)) +\n    theme_bw()\n\nplot_model_distance\n\n\n\n\n\n\n\n\nYeah this looks spot on.",
    "crumbs": [
      "Data Analysis",
      "Two Words",
      "Italian"
    ]
  },
  {
    "objectID": "Analyses/portuguese.html#person-semantics-model",
    "href": "Analyses/portuguese.html#person-semantics-model",
    "title": "Portuguese",
    "section": "",
    "text": "stan_data_person = list(W = W,\n                           R = R,\n                           K = K,\n                           distance = semantics_person %&gt;% \n                               arrange(word, referent, Item) %&gt;% \n                               pull(distance) %&gt;% \n                               array(dim=c(K, R, W)),\n                           N_resp = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               nrow(),\n                           Ki = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(Item) %&gt;% as.integer(),\n                           ref = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;% \n                               pull(referent) %&gt;% \n                               as.integer(),\n                           y = d_por %&gt;%\n                               arrange(Item) %&gt;%\n                               spread(word, y, fill = 0) %&gt;%\n                               select(aquel, ese, este) %&gt;%\n                               as.matrix() %&gt;%\n                               unname()\n)\n\nfit_pers = stan('model.stan',\n                data=stan_data_person,\n                seed=67651,\n                chains=4,\n                cores=4,\n                iter = 2000)\n\n\n\nChains that wind\n\nfit_pers %&gt;%\n    stan_trace(pars=c('stau_lap', 'ltau_lap'), inc_warmup=T)\n\n\n\n\n\n\n\n\nDistributional shape?\n\nfit_pers %&gt;%\n    stan_hist(pars=c('stau_lap', 'ltau_lap'))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nfit_summary(fit_pers, pars=c('stau_lap', 'ltau_lap'))\n\n  parameter       mean     se_mean       2.5%        50%      97.5%     Rhat\n1  stau_lap -0.5343092 0.001306373 -0.6401655 -0.5345664 -0.4199504 1.001711\n2  ltau_lap -0.2088843 0.002040674 -0.3835621 -0.2081466 -0.0423049 1.001005\n  n_eff Bulk_ESS Tail_ESS\n1  1874     1888     2138\n2  1795     1815     1925\n\n\nFace validity?\n\nS1_stan_pers = fit_pers %&gt;% \n    spread_draws(S1[Item, word, referent]) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    group_by(Item, word, referent) %&gt;%\n    summarise(S1=mean(S1)) %&gt;%\n    left_join(design)\n\n`summarise()` has grouped output by 'Item', 'word'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(Item, word, referent)`\n\nplt_pers = plot_S1(S1_stan_pers)\nplt_pers\n\n\n\n\n\n\n\n\nYeah that’s what’s expected.\n\n\n\n\ny_stan_person = fit_pers %&gt;% \n    spread_draws(y_rep[n, word]) %&gt;%\n    group_by(n, word) %&gt;%\n    summarise(person=mean(y_rep), \n              person_l=quantile(y_rep, 0.025),\n              person_h=quantile(y_rep, 0.975)) %&gt;%\n    mutate(word = case_when(word==1 ~ 'aquel',\n                            word==2 ~ 'ese',\n                            T ~ 'este')) %&gt;%\n    left_join(d_por %&gt;%\n                  arrange(Item) %&gt;%\n                  spread(word, y, fill = 0) %&gt;%\n                  select(aquel, ese, este) %&gt;%\n                  mutate(n=1:n()) %&gt;%\n                  gather(word, y, aquel:este))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\nJoining with `by = join_by(n, word)`\n\ny_stan_person %&gt;%\n    with(cor.test(person, y))\n\n\n    Pearson's product-moment correlation\n\ndata:  person and y\nt = 15.792, df = 46, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8589684 0.9539324\nsample estimates:\n      cor \n0.9188468 \n\nplot_model_person = y_stan_person %&gt;%\n    ggplot(aes(y, person)) +\n    geom_abline(intercept=1, linetype=2, alpha=0.5) +\n    geom_point() +\n    geom_linerange(aes(ymin=person_l, ymax=person_h)) +\n    theme_bw()\n\nplot_model_person + plot_model_distance\n\n\n\n\n\n\n\n\nYay that’s a somewhat okay fit.",
    "crumbs": [
      "Data Analysis",
      "Three Words",
      "Portuguese"
    ]
  },
  {
    "objectID": "index.html#inferred-parameters",
    "href": "index.html#inferred-parameters",
    "title": "Spatial Demonstratives",
    "section": "Inferred Parameters",
    "text": "Inferred Parameters\nFor computational ease, I technically inferred \\(\\log_2(\\tau)\\), which is normally distributed. One parameter each for speaker and listener. Negative values reflect upweighting the model predictions; whereas, positive values reflect backing off from the model predictions.\n\n\n\n\n\n\n\n\n\n\\(\\log_2(\\tau)\\)s are all negative, which would suggest that the model provides a good fit to the data. The line ranges are 95% credible intervals."
  },
  {
    "objectID": "index.html#group-level",
    "href": "index.html#group-level",
    "title": "Spatial Demonstratives",
    "section": "Group Level",
    "text": "Group Level\nSo here I’m plotting the Bayes Factor for the Distance Semantics / Person Semantics. Positive values suggest that Distance Semantics is a better fit and negative values suggest that Person Semantics is the better fit. If it’s close to 0 that means, there’s no strong evidence either way.\n\n\n\n\n\n\n\n\n\nAs far as I’m aware, other than Spanish, this agrees with the predictions Paula shared with me."
  },
  {
    "objectID": "index.html#individual-level",
    "href": "index.html#individual-level",
    "title": "Spatial Demonstratives",
    "section": "Individual Level",
    "text": "Individual Level\nWe can use the paramters from the group level model to calculate which semantics best explains each participant’s data. For the moment, I just did a crude assignment assigning an individual to whichever model had the greater likelihood without taking into account the strength of the evidence for the classification.\n\n\n\n\n\n\n\n\n\nThis basically confirms that same patterns as the group level analysis, but it’s notable that none of these experiments had participants unanimously fit better under one semantics than the other (despite the kind of description you might stumble across in a grammar)."
  }
]